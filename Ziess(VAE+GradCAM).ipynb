{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of VAE+GradCAM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXdCx7Ug7106i3usV2/rFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srijayjk/Computer-Vision/blob/main/Ziess(VAE%2BGradCAM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NfST5-_-xUz"
      },
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyTLIgAW-0MN"
      },
      "source": [
        "# Json file for Kaggle dataset\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASchIiVb-1z7"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkMdzmHm-3sa"
      },
      "source": [
        "#Unzip the dataset downloaded from kaggle\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('celeba-dataset.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in the data directory\n",
        "   zipObj.extractall('./data/')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJaEBgtj_GQn"
      },
      "source": [
        "# Import necessary libraries and Frameworks\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import os\n",
        "from glob import glob\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_laLKm4dxoUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e399eaa4-0656-4e6f-9473-c325326d0c03"
      },
      "source": [
        "print(np.__version__)\n",
        "print(tf.__version__)\n",
        "\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.19.5\n",
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqCigMYp_NJh",
        "outputId": "136bd6f5-af54-48b7-ef80-ed3c6b57f745"
      },
      "source": [
        "# Display number of images\n",
        "DATA_FOLDER = './data/img_align_celeba/'\n",
        "filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
        "NUM_IMAGES = len(filenames)\n",
        "print(\"Total number of images : \" + str(NUM_IMAGES))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of images : 202599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StUmCOfcrdnp"
      },
      "source": [
        "images_in_row = 1\n",
        "images_in_column = 1\n",
        "base_directory = '/content/data/img_align_celeba/img_align_celeba'\n",
        "\n",
        "paths_to_images = [os.path.join(base_directory, filename) \n",
        "                   for filename in os.listdir(base_directory)[:10]]\n",
        "\n",
        "image = cv2.imread(paths_to_images[5])\n",
        "current_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255\n",
        "height = image.shape[0]\n",
        "width = image.shape[1]\n",
        "channels = image.shape[2]\n",
        "print(\"Size of Image:\", height,'X', width)\n",
        "plt.figure(figsize = (5, 5))\n",
        "plt.axis('off')\n",
        "plt.imshow(current_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wcf67SL_Pl7",
        "outputId": "85eb46c5-ca12-4ae0-b6b3-fa0e1a9c4306"
      },
      "source": [
        "INPUT_DIM = (128,128,3) # Image dimension\n",
        "BATCH_SIZE = 256\n",
        "Z_DIM = 200 # Dimension of the latent vector (z)\n",
        "\n",
        "'''\n",
        "Image is rescaled \n",
        "Data is loaded using generator function flow_from_directory\n",
        "Class_mode is set as Input - which uses same input and output image\n",
        "'''\n",
        "data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, \n",
        "                                                                   target_size = INPUT_DIM[:2],\n",
        "                                                                   batch_size = BATCH_SIZE,\n",
        "                                                                   shuffle = True,\n",
        "                                                                   class_mode = 'input',\n",
        "                                                                   subset = 'training'\n",
        "                                                                   )"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 202599 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLoyo_Yd_UHe"
      },
      "source": [
        "\n",
        "'''\n",
        "Build Encoder\n",
        "With Input dimension, Output_dimension(Latent dimension)\n",
        "Name the each Conv layers for further calling\n",
        "\n",
        "build_vae_encoder\n",
        "\n",
        "Input arguments\n",
        "\n",
        "input_dim        : Dimesion of input image (Batch, width, height, channels)\n",
        "output_dim       : Dimension of Output i.e Latent space\n",
        "conv_filters     : Number of Conv filters. Expects elements in list []\n",
        "conv_kernel_size : Size of Kernel. Expects elements in list []\n",
        "conv_strides     : Strides in operation. Expects elements in list []\n",
        "\n",
        "Output arguments\n",
        "\n",
        "encoder_input           : Tensor with size (Batch, width, height, channels)\n",
        "encoder_output          : (None, Z_dimension)\n",
        "mean_mu                 : Mean \n",
        "log_var                 : Variance\n",
        "shape_before_flattening : Shape before flatenning or shape of last convolution layer\n",
        "Encoder Model           : Encoder model with input and all the intermediate layers\n",
        "'''\n",
        "def build_vae_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, \n",
        "                  conv_strides):\n",
        "  \n",
        "  global K\n",
        "  K.clear_session()\n",
        "  \n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
        "  x = encoder_input\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2D(filters = conv_filters[i], kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], padding = 'same', name = 'encoder_conv_' + str(i))(x)\n",
        "      x = LeakyReLU()(x)\n",
        "\n",
        "  # Required for reshaping latent vector while building Decoder\n",
        "  # int_shape return tuple unlike tf.shape which returns tensor\n",
        "  shape_before_flattening = K.int_shape(x)[1:] \n",
        "  x = Flatten()(x)\n",
        "\n",
        "  # Mean and Variance\n",
        "  mean_mu = Dense(output_dim, name = 'mu')(x)\n",
        "  log_var = Dense(output_dim, name = 'log_var')(x)\n",
        "\n",
        "  # Defining a function for sampling mean and variance\n",
        "  def sampling(args):\n",
        "    mean_mu, log_var = args\n",
        "    epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n",
        "    return mean_mu + K.exp(log_var/2)*epsilon   \n",
        "  \n",
        "  # Using a Keras Lambda Layer to include the sampling function as a layer \n",
        "  # in the model\n",
        "  encoder_output = Lambda(sampling, name='encoder_output')([mean_mu, log_var])\n",
        "\n",
        "  return encoder_input, encoder_output, mean_mu, log_var, shape_before_flattening, Model(encoder_input, encoder_output, name='Encoder'\n",
        "                                                                                )"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtPiGaPaDNb3"
      },
      "source": [
        "vae_encoder_input, vae_encoder_output,  mean_mu, log_var, vae_shape_before_flattening, vae_encoder  = build_vae_encoder(input_dim = INPUT_DIM,\n",
        "                                    output_dim = Z_DIM, \n",
        "                                    conv_filters = [32, 64, 64, 64],\n",
        "                                    conv_kernel_size = [3,3,3,3],\n",
        "                                    conv_strides = [2,2,2,2])\n",
        "\n",
        "vae_encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWDktZZtE1lR"
      },
      "source": [
        "\n",
        "'''\n",
        "build_decoder\n",
        "\n",
        "Input arguments\n",
        "\n",
        "input_dim        : Dimesion of Latent space. [(None, Z_dimension)]   \n",
        "conv_filters     : Number of Conv filters. Expects elements in list []\n",
        "conv_kernel_size : Size of Kernel. Expects elements in list []\n",
        "conv_strides     : Strides in operation. Expects elements in list []\n",
        "\n",
        "Output arguments\n",
        "\n",
        "Decoder_input           : Tensor with size [(None, Z_dimension)]   \n",
        "Decoder_output          : (Batch, width, height, channels)\n",
        "Decoder Model           : Decoder model with input and all the intermediate layers\n",
        "'''\n",
        "# Decoder\n",
        "def build_decoder(input_dim, shape_before_flattening, conv_filters, conv_kernel_size, \n",
        "                  conv_strides):\n",
        "\n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n",
        "\n",
        "  # To get an exact mirror image of the encoder\n",
        "  x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "  x = Reshape(shape_before_flattening)(x)\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2DTranspose(filters = conv_filters[i], \n",
        "                  kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], \n",
        "                  padding = 'same',\n",
        "                  name = 'decoder_conv_' + str(i)\n",
        "                  )(x)\n",
        "      \n",
        "      # Adding a sigmoid layer at the end to restrict the outputs \n",
        "      # between 0 and 1\n",
        "      if i < n_layers - 1:\n",
        "        x = LeakyReLU()(x)\n",
        "      else:\n",
        "        x = Activation('sigmoid')(x)\n",
        "\n",
        "  # Define model output\n",
        "  decoder_output = x\n",
        "\n",
        "  return decoder_input, decoder_output, Model(decoder_input, decoder_output, name='Decoder')"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAD2dvQmFGLg"
      },
      "source": [
        "vae_decoder_input, vae_decoder_output, vae_decoder = build_decoder(input_dim = Z_DIM,\n",
        "                                        shape_before_flattening = vae_shape_before_flattening,\n",
        "                                        conv_filters = [64,64,32,3],\n",
        "                                        conv_kernel_size = [3,3,3,3],\n",
        "                                        conv_strides = [2,2,2,2]\n",
        "                                        )\n",
        "vae_decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUXKcwxsFPQJ"
      },
      "source": [
        "# The input to the model will be the image fed to the encoder.\n",
        "vae_input = vae_encoder_input\n",
        "\n",
        "# Output will be the output of the decoder. The term - decoder(encoder_output) \n",
        "# combines the model by passing the encoder output to the input of the decoder.\n",
        "vae_output = vae_decoder(vae_encoder_output)\n",
        "\n",
        "# Input to the combined model will be the input to the encoder.\n",
        "# Output of the combined model will be the output of the decoder.\n",
        "vae_model = Model(vae_input, vae_output)\n",
        "\n",
        "vae_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe1Wr54UO9fe"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "N_EPOCHS = 2"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD3Z6TmOVN4C"
      },
      "source": [
        "# Reconstruction Loss for Decoder\n",
        "def r_loss(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
        "    \n",
        "# KL divergence loss for Encoder to force the unknown input distribution in Known Gaussian distribution\n",
        "def kl_loss(y_true, y_pred):\n",
        "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean_mu) - K.exp(log_var), axis = 1)\n",
        "    return kl_loss\n",
        "\n",
        "def total_loss(y_true, y_pred):\n",
        "    return r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2vY72ETVQMq"
      },
      "source": [
        "adam_optimizer = Adam(learning_rate = LEARNING_RATE)\n",
        "vae_model.compile(optimizer=adam_optimizer, loss = total_loss, metrics = [r_loss, kl_loss])\n"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoA1QAHiVQPn"
      },
      "source": [
        "vae_model.fit(data_flow, \n",
        "                        shuffle=True, \n",
        "                        epochs = N_EPOCHS, \n",
        "                        initial_epoch = 0, \n",
        "                        steps_per_epoch=NUM_IMAGES / BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wL5xqXHXhx_"
      },
      "source": [
        "Reconstrction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfQVJE7aXjbf"
      },
      "source": [
        "example_batch = next(data_flow)\n",
        "example_batch = example_batch[0]\n",
        "example_images = example_batch[:10]\n",
        "img_array = example_images[0]\n",
        "matplotlib.pyplot.imshow(img_array)\n",
        "img_array = tf.keras.preprocessing.image.img_to_array(img_array)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "res = vae_model.predict(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc1LfchPat6W"
      },
      "source": [
        "import matplotlib\n",
        "res = res.reshape((128,128,3))\n",
        "matplotlib.pyplot.imshow(res[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16AI42ORfLFa"
      },
      "source": [
        "#HeatMap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfNhP8ts3ap-"
      },
      "source": [
        "last_conv_layer_name = \"encoder_conv_3\"\n",
        "encoder_out = \"encoder_output\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stab2_diXkFn"
      },
      "source": [
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, encoder_out, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.get_layer(encoder_out).output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "    \n",
        "    grads = tape.gradient(preds, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = tf.matmul(last_conv_layer_output, pooled_grads[..., tf.newaxis])\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap, grads, pooled_grads, preds, last_conv_layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75zac5_boKp"
      },
      "source": [
        "heatmap, grads, pooled_grads, preds, last_conv_layer_output = make_gradcam_heatmap(img_array, vae_model, last_conv_layer_name, encoder_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYVW5ELJ55jA"
      },
      "source": [
        "print(preds)\n",
        "print(last_conv_layer_output)\n",
        "print(grads)\n",
        "print(pooled_grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxV3BT6XkIc"
      },
      "source": [
        "def save_and_display_gradcam(img, heatmap,cam_path=\"superimposed_img.jpg\", alpha=0.4):\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "    superimposed_img = cv2.addWeighted(jet_heatmap, 0.005, img, 0.995, 0)\n",
        "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "save_and_display_gradcam(img_array.squeeze(), heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}